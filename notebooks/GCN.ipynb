{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:16:07.770368Z",
     "start_time": "2025-11-22T09:15:59.381638Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from scripts.data_formatting import SmilesDataset\n",
    "from scripts.downstream import get_prediction_smiles, split_batch_by_molecule\n",
    "from scripts.nn_models import GCN"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ae6f9835a62a1d5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:16:07.796828Z",
     "start_time": "2025-11-22T09:16:07.776926Z"
    }
   },
   "source": [
    "# raw dataset\n",
    "df = pd.read_csv('../datasets/13k_All_Manual.csv')\n",
    "smiles_list = df['SMILES Labelled'].tolist()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ced30a7c1f93e96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:16:24.879869Z",
     "start_time": "2025-11-22T09:16:07.804300Z"
    }
   },
   "source": [
    "# 70/15/15 train/test/val split\n",
    "train_smiles, test_smiles = train_test_split(smiles_list, test_size=0.15, random_state=42)\n",
    "train_smiles, val_smiles = train_test_split(train_smiles, test_size=0.1765, random_state=42)\n",
    "\n",
    "# dataset objects\n",
    "train_dataset = SmilesDataset(train_smiles)\n",
    "test_dataset  = SmilesDataset(test_smiles)\n",
    "val_dataset   = SmilesDataset(val_smiles)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "ff6f6b800cf8f569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:16:25.517665Z",
     "start_time": "2025-11-22T09:16:25.026991Z"
    }
   },
   "source": [
    "# class weights based on train split\n",
    "labels = []\n",
    "for batch in train_loader:\n",
    "    labels.append(batch.y.argmax(dim=1))\n",
    "labels = torch.cat(labels)\n",
    "class_counts = torch.bincount(labels)\n",
    "class_weights = labels.size(0) / (len(class_counts) * class_counts.float())\n",
    "\n",
    "# model instance\n",
    "model = GCN(input_dim=9, hidden_dim=128, output_dim=5, num_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T10:39:57.709571Z",
     "start_time": "2025-11-22T09:16:25.529264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "counter = 0\n",
    "patience = 15\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = criterion(out, batch.y.argmax(dim=1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds, true = split_batch_by_molecule(out, batch)\n",
    "            for p, t in zip(preds, true):\n",
    "                if torch.equal(p, t):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # validation accuracy\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        counter = 0\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"../models/gcn/gcn_{epoch+1}.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Training stopped at epoch {epoch+1}.\")\n",
    "            break"
   ],
   "id": "34ff896c3fcd7c4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6085 | Val Loss: 0.5308 | Val Acc: 15.55%\n",
      "Epoch 2 | Train Loss: 0.5183 | Val Loss: 0.4976 | Val Acc: 19.51%\n",
      "Epoch 3 | Train Loss: 0.5001 | Val Loss: 0.4905 | Val Acc: 19.19%\n",
      "Epoch 4 | Train Loss: 0.4954 | Val Loss: 0.4820 | Val Acc: 19.40%\n",
      "Epoch 98 | Train Loss: 0.3165 | Val Loss: 0.3128 | Val Acc: 35.28%\n",
      "Epoch 99 | Train Loss: 0.3146 | Val Loss: 0.3152 | Val Acc: 35.76%\n",
      "Epoch 100 | Train Loss: 0.3135 | Val Loss: 0.3058 | Val Acc: 35.76%\n",
      "Epoch 101 | Train Loss: 0.3106 | Val Loss: 0.3078 | Val Acc: 35.17%\n",
      "Epoch 102 | Train Loss: 0.3136 | Val Loss: 0.3105 | Val Acc: 37.31%\n",
      "Epoch 103 | Train Loss: 0.3109 | Val Loss: 0.3094 | Val Acc: 34.63%\n",
      "Epoch 104 | Train Loss: 0.3106 | Val Loss: 0.3131 | Val Acc: 35.86%\n",
      "Epoch 105 | Train Loss: 0.3105 | Val Loss: 0.3085 | Val Acc: 35.38%\n",
      "Epoch 106 | Train Loss: 0.3087 | Val Loss: 0.3255 | Val Acc: 34.85%\n",
      "Epoch 107 | Train Loss: 0.3095 | Val Loss: 0.2995 | Val Acc: 35.86%\n",
      "Epoch 108 | Train Loss: 0.3095 | Val Loss: 0.3001 | Val Acc: 35.60%\n",
      "Epoch 109 | Train Loss: 0.3086 | Val Loss: 0.3003 | Val Acc: 35.49%\n",
      "Epoch 110 | Train Loss: 0.3087 | Val Loss: 0.2963 | Val Acc: 35.81%\n",
      "Epoch 111 | Train Loss: 0.3098 | Val Loss: 0.3258 | Val Acc: 36.40%\n",
      "Epoch 112 | Train Loss: 0.3082 | Val Loss: 0.3023 | Val Acc: 35.44%\n",
      "Epoch 113 | Train Loss: 0.3067 | Val Loss: 0.3023 | Val Acc: 35.11%\n",
      "Epoch 114 | Train Loss: 0.3067 | Val Loss: 0.2976 | Val Acc: 36.08%\n",
      "Epoch 115 | Train Loss: 0.3075 | Val Loss: 0.3022 | Val Acc: 37.84%\n",
      "Epoch 116 | Train Loss: 0.3074 | Val Loss: 0.2954 | Val Acc: 37.04%\n",
      "Epoch 117 | Train Loss: 0.3051 | Val Loss: 0.3018 | Val Acc: 36.83%\n",
      "Epoch 118 | Train Loss: 0.3035 | Val Loss: 0.2999 | Val Acc: 37.47%\n",
      "Epoch 119 | Train Loss: 0.3040 | Val Loss: 0.2973 | Val Acc: 38.21%\n",
      "Epoch 120 | Train Loss: 0.3039 | Val Loss: 0.2981 | Val Acc: 38.27%\n",
      "Epoch 121 | Train Loss: 0.3021 | Val Loss: 0.2998 | Val Acc: 37.89%\n",
      "Epoch 122 | Train Loss: 0.3033 | Val Loss: 0.3184 | Val Acc: 36.83%\n",
      "Epoch 123 | Train Loss: 0.3015 | Val Loss: 0.2943 | Val Acc: 37.20%\n",
      "Epoch 124 | Train Loss: 0.3014 | Val Loss: 0.3155 | Val Acc: 34.95%\n",
      "Epoch 125 | Train Loss: 0.3005 | Val Loss: 0.3064 | Val Acc: 36.99%\n",
      "Epoch 126 | Train Loss: 0.3005 | Val Loss: 0.2916 | Val Acc: 37.95%\n",
      "Epoch 127 | Train Loss: 0.3016 | Val Loss: 0.3196 | Val Acc: 38.05%\n",
      "Epoch 128 | Train Loss: 0.2995 | Val Loss: 0.2897 | Val Acc: 37.04%\n",
      "Epoch 129 | Train Loss: 0.2993 | Val Loss: 0.3162 | Val Acc: 36.45%\n",
      "Epoch 130 | Train Loss: 0.2983 | Val Loss: 0.3109 | Val Acc: 37.31%\n",
      "Epoch 131 | Train Loss: 0.3003 | Val Loss: 0.2880 | Val Acc: 37.52%\n",
      "Epoch 132 | Train Loss: 0.2990 | Val Loss: 0.3125 | Val Acc: 36.45%\n",
      "Epoch 133 | Train Loss: 0.2985 | Val Loss: 0.2919 | Val Acc: 36.66%\n",
      "Epoch 134 | Train Loss: 0.2974 | Val Loss: 0.2964 | Val Acc: 36.72%\n",
      "Epoch 135 | Train Loss: 0.2980 | Val Loss: 0.2856 | Val Acc: 37.20%\n",
      "Epoch 136 | Train Loss: 0.2946 | Val Loss: 0.2861 | Val Acc: 37.95%\n",
      "Epoch 137 | Train Loss: 0.2964 | Val Loss: 0.2939 | Val Acc: 35.22%\n",
      "Epoch 138 | Train Loss: 0.2951 | Val Loss: 0.2912 | Val Acc: 37.63%\n",
      "Epoch 139 | Train Loss: 0.2953 | Val Loss: 0.2903 | Val Acc: 37.41%\n",
      "Epoch 140 | Train Loss: 0.2950 | Val Loss: 0.2856 | Val Acc: 36.93%\n",
      "Epoch 141 | Train Loss: 0.2943 | Val Loss: 0.3019 | Val Acc: 39.07%\n",
      "Epoch 142 | Train Loss: 0.2959 | Val Loss: 0.2972 | Val Acc: 36.24%\n",
      "Epoch 143 | Train Loss: 0.2955 | Val Loss: 0.2863 | Val Acc: 36.99%\n",
      "Epoch 144 | Train Loss: 0.2955 | Val Loss: 0.2923 | Val Acc: 36.93%\n",
      "Epoch 145 | Train Loss: 0.2937 | Val Loss: 0.3137 | Val Acc: 37.20%\n",
      "Epoch 146 | Train Loss: 0.2937 | Val Loss: 0.2971 | Val Acc: 39.76%\n",
      "Epoch 147 | Train Loss: 0.2925 | Val Loss: 0.2886 | Val Acc: 35.86%\n",
      "Epoch 148 | Train Loss: 0.2942 | Val Loss: 0.3180 | Val Acc: 38.91%\n",
      "Epoch 149 | Train Loss: 0.2926 | Val Loss: 0.2834 | Val Acc: 38.80%\n",
      "Epoch 150 | Train Loss: 0.2920 | Val Loss: 0.2887 | Val Acc: 36.66%\n",
      "Epoch 151 | Train Loss: 0.2914 | Val Loss: 0.2924 | Val Acc: 39.12%\n",
      "Epoch 152 | Train Loss: 0.2908 | Val Loss: 0.2887 | Val Acc: 40.41%\n",
      "Epoch 153 | Train Loss: 0.2945 | Val Loss: 0.2788 | Val Acc: 38.05%\n",
      "Epoch 154 | Train Loss: 0.2905 | Val Loss: 0.2793 | Val Acc: 39.18%\n",
      "Epoch 155 | Train Loss: 0.2891 | Val Loss: 0.2832 | Val Acc: 39.55%\n",
      "Epoch 156 | Train Loss: 0.2899 | Val Loss: 0.2991 | Val Acc: 37.95%\n",
      "Epoch 157 | Train Loss: 0.2925 | Val Loss: 0.2899 | Val Acc: 39.34%\n",
      "Epoch 158 | Train Loss: 0.2890 | Val Loss: 0.2799 | Val Acc: 40.51%\n",
      "Epoch 159 | Train Loss: 0.2874 | Val Loss: 0.2818 | Val Acc: 38.86%\n",
      "Epoch 160 | Train Loss: 0.2893 | Val Loss: 0.2847 | Val Acc: 38.75%\n",
      "Epoch 161 | Train Loss: 0.2881 | Val Loss: 0.2935 | Val Acc: 40.78%\n",
      "Epoch 162 | Train Loss: 0.2875 | Val Loss: 0.2838 | Val Acc: 37.84%\n",
      "Epoch 163 | Train Loss: 0.2866 | Val Loss: 0.3018 | Val Acc: 39.44%\n",
      "Epoch 164 | Train Loss: 0.2859 | Val Loss: 0.2757 | Val Acc: 39.28%\n",
      "Epoch 165 | Train Loss: 0.2877 | Val Loss: 0.2857 | Val Acc: 39.60%\n",
      "Epoch 166 | Train Loss: 0.2854 | Val Loss: 0.2807 | Val Acc: 38.70%\n",
      "Epoch 167 | Train Loss: 0.2888 | Val Loss: 0.2779 | Val Acc: 38.80%\n",
      "Epoch 168 | Train Loss: 0.2848 | Val Loss: 0.2750 | Val Acc: 39.50%\n",
      "Epoch 169 | Train Loss: 0.2847 | Val Loss: 0.2891 | Val Acc: 40.41%\n",
      "Epoch 170 | Train Loss: 0.2847 | Val Loss: 0.2824 | Val Acc: 36.45%\n",
      "Epoch 171 | Train Loss: 0.2860 | Val Loss: 0.2744 | Val Acc: 40.19%\n",
      "Epoch 172 | Train Loss: 0.2850 | Val Loss: 0.2939 | Val Acc: 38.11%\n",
      "Epoch 173 | Train Loss: 0.2833 | Val Loss: 0.2843 | Val Acc: 39.07%\n",
      "Epoch 174 | Train Loss: 0.2845 | Val Loss: 0.2723 | Val Acc: 38.75%\n",
      "Epoch 175 | Train Loss: 0.2816 | Val Loss: 0.2834 | Val Acc: 39.34%\n",
      "Epoch 176 | Train Loss: 0.2835 | Val Loss: 0.2718 | Val Acc: 40.57%\n",
      "Epoch 177 | Train Loss: 0.2822 | Val Loss: 0.2767 | Val Acc: 39.60%\n",
      "Epoch 178 | Train Loss: 0.2825 | Val Loss: 0.2806 | Val Acc: 39.07%\n",
      "Epoch 179 | Train Loss: 0.2825 | Val Loss: 0.2819 | Val Acc: 40.25%\n",
      "Epoch 180 | Train Loss: 0.2816 | Val Loss: 0.2764 | Val Acc: 41.42%\n",
      "Epoch 181 | Train Loss: 0.2822 | Val Loss: 0.2809 | Val Acc: 36.56%\n",
      "Epoch 182 | Train Loss: 0.2830 | Val Loss: 0.2886 | Val Acc: 38.96%\n",
      "Epoch 183 | Train Loss: 0.2787 | Val Loss: 0.2719 | Val Acc: 40.46%\n",
      "Epoch 184 | Train Loss: 0.2817 | Val Loss: 0.2722 | Val Acc: 40.51%\n",
      "Epoch 185 | Train Loss: 0.2815 | Val Loss: 0.2794 | Val Acc: 39.12%\n",
      "Epoch 186 | Train Loss: 0.2825 | Val Loss: 0.2745 | Val Acc: 40.25%\n",
      "Epoch 187 | Train Loss: 0.2787 | Val Loss: 0.2849 | Val Acc: 40.73%\n",
      "Epoch 188 | Train Loss: 0.2829 | Val Loss: 0.2719 | Val Acc: 39.60%\n",
      "Epoch 189 | Train Loss: 0.2814 | Val Loss: 0.2710 | Val Acc: 40.30%\n",
      "Epoch 190 | Train Loss: 0.2776 | Val Loss: 0.2800 | Val Acc: 39.82%\n",
      "Epoch 191 | Train Loss: 0.2806 | Val Loss: 0.2807 | Val Acc: 39.12%\n",
      "Epoch 192 | Train Loss: 0.2806 | Val Loss: 0.2733 | Val Acc: 39.34%\n",
      "Epoch 193 | Train Loss: 0.2765 | Val Loss: 0.3020 | Val Acc: 39.07%\n",
      "Epoch 194 | Train Loss: 0.2803 | Val Loss: 0.2744 | Val Acc: 41.53%\n",
      "Epoch 195 | Train Loss: 0.2781 | Val Loss: 0.2685 | Val Acc: 40.35%\n",
      "Epoch 196 | Train Loss: 0.2793 | Val Loss: 0.2709 | Val Acc: 40.03%\n",
      "Epoch 197 | Train Loss: 0.2792 | Val Loss: 0.2752 | Val Acc: 40.25%\n",
      "Epoch 198 | Train Loss: 0.2752 | Val Loss: 0.2662 | Val Acc: 39.07%\n",
      "Epoch 199 | Train Loss: 0.2791 | Val Loss: 0.2754 | Val Acc: 40.03%\n",
      "Epoch 200 | Train Loss: 0.2762 | Val Loss: 0.2697 | Val Acc: 41.21%\n",
      "Epoch 201 | Train Loss: 0.2758 | Val Loss: 0.2711 | Val Acc: 42.01%\n",
      "Epoch 202 | Train Loss: 0.2761 | Val Loss: 0.2705 | Val Acc: 38.70%\n",
      "Epoch 203 | Train Loss: 0.2754 | Val Loss: 0.2750 | Val Acc: 40.19%\n",
      "Epoch 204 | Train Loss: 0.2780 | Val Loss: 0.2819 | Val Acc: 41.74%\n",
      "Epoch 205 | Train Loss: 0.2746 | Val Loss: 0.2696 | Val Acc: 39.02%\n",
      "Epoch 206 | Train Loss: 0.2759 | Val Loss: 0.2722 | Val Acc: 41.10%\n",
      "Epoch 207 | Train Loss: 0.2746 | Val Loss: 0.2757 | Val Acc: 40.94%\n",
      "Epoch 208 | Train Loss: 0.2747 | Val Loss: 0.2804 | Val Acc: 39.07%\n",
      "Epoch 209 | Train Loss: 0.2750 | Val Loss: 0.2759 | Val Acc: 40.62%\n",
      "Epoch 210 | Train Loss: 0.2761 | Val Loss: 0.2646 | Val Acc: 41.21%\n",
      "Epoch 211 | Train Loss: 0.2785 | Val Loss: 0.2646 | Val Acc: 40.83%\n",
      "Epoch 212 | Train Loss: 0.2757 | Val Loss: 0.2644 | Val Acc: 40.46%\n",
      "Epoch 213 | Train Loss: 0.2747 | Val Loss: 0.2853 | Val Acc: 39.71%\n",
      "Epoch 214 | Train Loss: 0.2730 | Val Loss: 0.2763 | Val Acc: 40.03%\n",
      "Epoch 215 | Train Loss: 0.2752 | Val Loss: 0.2639 | Val Acc: 41.90%\n",
      "Epoch 216 | Train Loss: 0.2711 | Val Loss: 0.2714 | Val Acc: 41.21%\n",
      "Epoch 217 | Train Loss: 0.2729 | Val Loss: 0.2681 | Val Acc: 41.96%\n",
      "Epoch 218 | Train Loss: 0.2719 | Val Loss: 0.2689 | Val Acc: 42.44%\n",
      "Epoch 219 | Train Loss: 0.2738 | Val Loss: 0.2622 | Val Acc: 41.37%\n",
      "Epoch 220 | Train Loss: 0.2741 | Val Loss: 0.2664 | Val Acc: 40.19%\n",
      "Epoch 221 | Train Loss: 0.2737 | Val Loss: 0.2722 | Val Acc: 40.78%\n",
      "Epoch 222 | Train Loss: 0.2723 | Val Loss: 0.2751 | Val Acc: 39.28%\n",
      "Epoch 223 | Train Loss: 0.2702 | Val Loss: 0.2756 | Val Acc: 41.31%\n",
      "Epoch 224 | Train Loss: 0.2751 | Val Loss: 0.2624 | Val Acc: 42.54%\n",
      "Epoch 225 | Train Loss: 0.2731 | Val Loss: 0.2696 | Val Acc: 40.51%\n",
      "Epoch 226 | Train Loss: 0.2722 | Val Loss: 0.2653 | Val Acc: 39.93%\n",
      "Epoch 227 | Train Loss: 0.2737 | Val Loss: 0.2660 | Val Acc: 40.67%\n",
      "Epoch 228 | Train Loss: 0.2717 | Val Loss: 0.2688 | Val Acc: 40.94%\n",
      "Epoch 229 | Train Loss: 0.2717 | Val Loss: 0.2682 | Val Acc: 41.37%\n",
      "Epoch 230 | Train Loss: 0.2709 | Val Loss: 0.2614 | Val Acc: 41.53%\n",
      "Epoch 231 | Train Loss: 0.2706 | Val Loss: 0.2616 | Val Acc: 42.38%\n",
      "Epoch 232 | Train Loss: 0.2710 | Val Loss: 0.2614 | Val Acc: 43.35%\n",
      "Epoch 233 | Train Loss: 0.2724 | Val Loss: 0.2703 | Val Acc: 42.17%\n",
      "Epoch 234 | Train Loss: 0.2691 | Val Loss: 0.2754 | Val Acc: 43.24%\n",
      "Epoch 235 | Train Loss: 0.2714 | Val Loss: 0.2659 | Val Acc: 42.38%\n",
      "Epoch 236 | Train Loss: 0.2701 | Val Loss: 0.2705 | Val Acc: 39.12%\n",
      "Epoch 237 | Train Loss: 0.2688 | Val Loss: 0.2633 | Val Acc: 41.80%\n",
      "Epoch 238 | Train Loss: 0.2712 | Val Loss: 0.2622 | Val Acc: 41.10%\n",
      "Epoch 239 | Train Loss: 0.2726 | Val Loss: 0.2683 | Val Acc: 42.38%\n",
      "Epoch 240 | Train Loss: 0.2683 | Val Loss: 0.2626 | Val Acc: 40.83%\n",
      "Epoch 241 | Train Loss: 0.2684 | Val Loss: 0.2803 | Val Acc: 41.31%\n",
      "Epoch 242 | Train Loss: 0.2707 | Val Loss: 0.2646 | Val Acc: 42.70%\n",
      "Epoch 243 | Train Loss: 0.2670 | Val Loss: 0.2646 | Val Acc: 41.10%\n",
      "Epoch 244 | Train Loss: 0.2686 | Val Loss: 0.2638 | Val Acc: 40.83%\n",
      "Epoch 245 | Train Loss: 0.2709 | Val Loss: 0.2640 | Val Acc: 42.06%\n",
      "Training stopped at epoch 245.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T04:58:32.159443Z",
     "start_time": "2025-11-22T04:58:29.315740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_PATH = '../models/gcn/gcn_217.pt'\n",
    "OUTPUT_CSV = '../results/gcn_test_eval.csv'\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "smiles_original = []\n",
    "smiles_predicted = []\n",
    "smiles_matched = []\n",
    "\n",
    "# test evaluation\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.argmax(dim=1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds, true = split_batch_by_molecule(out, batch)\n",
    "        for p, t in zip(preds, true):\n",
    "            if torch.equal(p, t):\n",
    "                smiles_matched.append(True)\n",
    "                correct += 1\n",
    "            else:\n",
    "                smiles_matched.append(False)\n",
    "            total += 1\n",
    "\n",
    "        smiles_original.extend(batch.smiles)\n",
    "        smiles_predicted.extend(get_prediction_smiles(preds, batch.smiles))\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = correct / total\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# saves test predictions to .csv file\n",
    "test_df = pd.DataFrame({\n",
    "    'SMILES Original': smiles_original,\n",
    "    'SMILES Predicted': smiles_predicted,\n",
    "    'SMILES Matched': smiles_matched\n",
    "})\n",
    "test_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved predictions to {OUTPUT_CSV}.\")"
   ],
   "id": "8147a150473fa8ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2733\n",
      "Test Accuracy: 43.45%\n",
      "Saved predictions to ../results/gcn_test_eval.csv.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dd1683aacc060128",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
