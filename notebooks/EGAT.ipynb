{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:45:24.992694Z",
     "start_time": "2025-11-22T09:45:18.933027Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from scripts.data_formatting import SmilesDataset\n",
    "from scripts.downstream import get_prediction_smiles, split_batch_by_molecule\n",
    "from scripts.nn_models import EGAT"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ae6f9835a62a1d5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:45:26.396241Z",
     "start_time": "2025-11-22T09:45:26.378928Z"
    }
   },
   "source": [
    "# raw dataset\n",
    "df = pd.read_csv('../datasets/13k_All_Manual.csv')\n",
    "smiles_list = df['SMILES Labelled'].tolist()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ced30a7c1f93e96f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:45:39.568586Z",
     "start_time": "2025-11-22T09:45:27.052531Z"
    }
   },
   "source": [
    "# 70/15/15 train/test/val split\n",
    "train_smiles, test_smiles = train_test_split(smiles_list, test_size=0.15, random_state=42)\n",
    "train_smiles, val_smiles = train_test_split(train_smiles, test_size=0.1765, random_state=42)\n",
    "\n",
    "# dataset objects\n",
    "train_dataset = SmilesDataset(train_smiles)\n",
    "test_dataset  = SmilesDataset(test_smiles)\n",
    "val_dataset   = SmilesDataset(val_smiles)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:45:41.295971Z",
     "start_time": "2025-11-22T09:45:41.192049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model instance\n",
    "model = EGAT(input_dim=9, hidden_dim=128, output_dim=5, edge_dim=8, heads=4, num_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ],
   "id": "3dba33408e5be78a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T13:38:19.251091Z",
     "start_time": "2025-11-22T09:45:42.397828Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.5418 | Val Loss: 0.4602 | Val Acc: 16.30%\n",
      "Epoch 2 | Train Loss: 0.4449 | Val Loss: 0.4275 | Val Acc: 20.95%\n",
      "Epoch 3 | Train Loss: 0.4202 | Val Loss: 0.4042 | Val Acc: 22.93%\n",
      "Epoch 4 | Train Loss: 0.3980 | Val Loss: 0.3827 | Val Acc: 23.89%\n",
      "Epoch 5 | Train Loss: 0.3766 | Val Loss: 0.3643 | Val Acc: 26.72%\n",
      "Epoch 6 | Train Loss: 0.3600 | Val Loss: 0.3522 | Val Acc: 27.95%\n",
      "Epoch 7 | Train Loss: 0.3491 | Val Loss: 0.3415 | Val Acc: 28.22%\n",
      "Epoch 8 | Train Loss: 0.3398 | Val Loss: 0.3313 | Val Acc: 28.86%\n",
      "Epoch 9 | Train Loss: 0.3321 | Val Loss: 0.3293 | Val Acc: 30.63%\n",
      "Epoch 10 | Train Loss: 0.3261 | Val Loss: 0.3187 | Val Acc: 31.00%\n",
      "Epoch 11 | Train Loss: 0.3201 | Val Loss: 0.3139 | Val Acc: 31.37%\n",
      "Epoch 12 | Train Loss: 0.3150 | Val Loss: 0.3075 | Val Acc: 30.57%\n",
      "Epoch 13 | Train Loss: 0.3102 | Val Loss: 0.3065 | Val Acc: 31.37%\n",
      "Epoch 14 | Train Loss: 0.3046 | Val Loss: 0.3015 | Val Acc: 32.28%\n",
      "Epoch 15 | Train Loss: 0.3038 | Val Loss: 0.3009 | Val Acc: 31.59%\n",
      "Epoch 16 | Train Loss: 0.3010 | Val Loss: 0.2954 | Val Acc: 32.87%\n",
      "Epoch 17 | Train Loss: 0.2974 | Val Loss: 0.2924 | Val Acc: 33.03%\n",
      "Epoch 18 | Train Loss: 0.2940 | Val Loss: 0.2929 | Val Acc: 35.01%\n",
      "Epoch 19 | Train Loss: 0.2928 | Val Loss: 0.2924 | Val Acc: 35.17%\n",
      "Epoch 20 | Train Loss: 0.2917 | Val Loss: 0.2880 | Val Acc: 34.63%\n",
      "Epoch 21 | Train Loss: 0.2892 | Val Loss: 0.2845 | Val Acc: 35.97%\n",
      "Epoch 22 | Train Loss: 0.2879 | Val Loss: 0.2820 | Val Acc: 35.60%\n",
      "Epoch 23 | Train Loss: 0.2882 | Val Loss: 0.2859 | Val Acc: 35.81%\n",
      "Epoch 24 | Train Loss: 0.2864 | Val Loss: 0.2851 | Val Acc: 38.21%\n",
      "Epoch 25 | Train Loss: 0.2855 | Val Loss: 0.2771 | Val Acc: 37.95%\n",
      "Epoch 26 | Train Loss: 0.2826 | Val Loss: 0.2848 | Val Acc: 36.88%\n",
      "Epoch 27 | Train Loss: 0.2815 | Val Loss: 0.2753 | Val Acc: 38.54%\n",
      "Epoch 28 | Train Loss: 0.2794 | Val Loss: 0.2737 | Val Acc: 38.70%\n",
      "Epoch 29 | Train Loss: 0.2796 | Val Loss: 0.2752 | Val Acc: 37.89%\n",
      "Epoch 30 | Train Loss: 0.2777 | Val Loss: 0.2737 | Val Acc: 37.52%\n",
      "Epoch 31 | Train Loss: 0.2762 | Val Loss: 0.2736 | Val Acc: 38.27%\n",
      "Epoch 32 | Train Loss: 0.2774 | Val Loss: 0.2836 | Val Acc: 38.38%\n",
      "Epoch 33 | Train Loss: 0.2758 | Val Loss: 0.2696 | Val Acc: 39.87%\n",
      "Epoch 34 | Train Loss: 0.2744 | Val Loss: 0.2690 | Val Acc: 39.39%\n",
      "Epoch 35 | Train Loss: 0.2738 | Val Loss: 0.2727 | Val Acc: 39.71%\n",
      "Epoch 36 | Train Loss: 0.2748 | Val Loss: 0.2681 | Val Acc: 38.91%\n",
      "Epoch 37 | Train Loss: 0.2712 | Val Loss: 0.2710 | Val Acc: 39.44%\n",
      "Epoch 38 | Train Loss: 0.2703 | Val Loss: 0.2681 | Val Acc: 40.03%\n",
      "Epoch 39 | Train Loss: 0.2693 | Val Loss: 0.2655 | Val Acc: 40.30%\n",
      "Epoch 40 | Train Loss: 0.2690 | Val Loss: 0.2659 | Val Acc: 39.93%\n",
      "Epoch 41 | Train Loss: 0.2689 | Val Loss: 0.2672 | Val Acc: 41.64%\n",
      "Epoch 42 | Train Loss: 0.2696 | Val Loss: 0.2666 | Val Acc: 40.83%\n",
      "Epoch 43 | Train Loss: 0.2664 | Val Loss: 0.2691 | Val Acc: 40.14%\n",
      "Epoch 44 | Train Loss: 0.2668 | Val Loss: 0.2646 | Val Acc: 41.05%\n",
      "Epoch 45 | Train Loss: 0.2651 | Val Loss: 0.2636 | Val Acc: 40.25%\n",
      "Epoch 46 | Train Loss: 0.2660 | Val Loss: 0.2611 | Val Acc: 42.01%\n",
      "Epoch 47 | Train Loss: 0.2649 | Val Loss: 0.2748 | Val Acc: 40.94%\n",
      "Epoch 48 | Train Loss: 0.2652 | Val Loss: 0.2614 | Val Acc: 40.51%\n",
      "Epoch 49 | Train Loss: 0.2646 | Val Loss: 0.2798 | Val Acc: 40.57%\n",
      "Epoch 50 | Train Loss: 0.2625 | Val Loss: 0.2625 | Val Acc: 42.06%\n",
      "Epoch 51 | Train Loss: 0.2618 | Val Loss: 0.2607 | Val Acc: 41.42%\n",
      "Epoch 52 | Train Loss: 0.2617 | Val Loss: 0.2629 | Val Acc: 42.49%\n",
      "Epoch 53 | Train Loss: 0.2620 | Val Loss: 0.2603 | Val Acc: 41.80%\n",
      "Epoch 54 | Train Loss: 0.2599 | Val Loss: 0.2578 | Val Acc: 42.17%\n",
      "Epoch 55 | Train Loss: 0.2596 | Val Loss: 0.2580 | Val Acc: 42.65%\n",
      "Epoch 56 | Train Loss: 0.2604 | Val Loss: 0.2592 | Val Acc: 42.12%\n",
      "Epoch 57 | Train Loss: 0.2588 | Val Loss: 0.2558 | Val Acc: 42.33%\n",
      "Epoch 58 | Train Loss: 0.2579 | Val Loss: 0.2577 | Val Acc: 42.12%\n",
      "Epoch 59 | Train Loss: 0.2588 | Val Loss: 0.2546 | Val Acc: 42.97%\n",
      "Epoch 60 | Train Loss: 0.2570 | Val Loss: 0.2541 | Val Acc: 43.03%\n",
      "Epoch 61 | Train Loss: 0.2571 | Val Loss: 0.2585 | Val Acc: 42.49%\n",
      "Epoch 62 | Train Loss: 0.2560 | Val Loss: 0.2603 | Val Acc: 42.17%\n",
      "Epoch 63 | Train Loss: 0.2576 | Val Loss: 0.2541 | Val Acc: 43.03%\n",
      "Epoch 64 | Train Loss: 0.2554 | Val Loss: 0.2515 | Val Acc: 43.83%\n",
      "Epoch 65 | Train Loss: 0.2563 | Val Loss: 0.2549 | Val Acc: 43.40%\n",
      "Epoch 66 | Train Loss: 0.2548 | Val Loss: 0.2515 | Val Acc: 43.35%\n",
      "Epoch 67 | Train Loss: 0.2537 | Val Loss: 0.2548 | Val Acc: 44.47%\n",
      "Epoch 68 | Train Loss: 0.2542 | Val Loss: 0.2600 | Val Acc: 43.67%\n",
      "Epoch 69 | Train Loss: 0.2533 | Val Loss: 0.2515 | Val Acc: 44.20%\n",
      "Epoch 70 | Train Loss: 0.2541 | Val Loss: 0.2513 | Val Acc: 43.19%\n",
      "Epoch 71 | Train Loss: 0.2526 | Val Loss: 0.2528 | Val Acc: 44.09%\n",
      "Epoch 72 | Train Loss: 0.2519 | Val Loss: 0.2517 | Val Acc: 43.72%\n",
      "Epoch 73 | Train Loss: 0.2517 | Val Loss: 0.2555 | Val Acc: 44.36%\n",
      "Epoch 74 | Train Loss: 0.2510 | Val Loss: 0.2499 | Val Acc: 44.79%\n",
      "Epoch 75 | Train Loss: 0.2511 | Val Loss: 0.2508 | Val Acc: 44.79%\n",
      "Epoch 76 | Train Loss: 0.2515 | Val Loss: 0.2480 | Val Acc: 45.22%\n",
      "Epoch 77 | Train Loss: 0.2500 | Val Loss: 0.2471 | Val Acc: 45.06%\n",
      "Epoch 78 | Train Loss: 0.2494 | Val Loss: 0.2502 | Val Acc: 44.58%\n",
      "Epoch 79 | Train Loss: 0.2490 | Val Loss: 0.2504 | Val Acc: 44.95%\n",
      "Epoch 80 | Train Loss: 0.2487 | Val Loss: 0.2483 | Val Acc: 44.79%\n",
      "Epoch 81 | Train Loss: 0.2493 | Val Loss: 0.2465 | Val Acc: 45.16%\n",
      "Epoch 82 | Train Loss: 0.2479 | Val Loss: 0.2511 | Val Acc: 46.13%\n",
      "Epoch 83 | Train Loss: 0.2485 | Val Loss: 0.2474 | Val Acc: 44.95%\n",
      "Epoch 84 | Train Loss: 0.2470 | Val Loss: 0.2499 | Val Acc: 45.48%\n",
      "Epoch 85 | Train Loss: 0.2468 | Val Loss: 0.2458 | Val Acc: 45.27%\n",
      "Epoch 86 | Train Loss: 0.2474 | Val Loss: 0.2501 | Val Acc: 45.16%\n",
      "Epoch 87 | Train Loss: 0.2473 | Val Loss: 0.2483 | Val Acc: 45.16%\n",
      "Epoch 88 | Train Loss: 0.2457 | Val Loss: 0.2453 | Val Acc: 45.54%\n",
      "Epoch 89 | Train Loss: 0.2471 | Val Loss: 0.2462 | Val Acc: 43.72%\n",
      "Epoch 90 | Train Loss: 0.2462 | Val Loss: 0.2484 | Val Acc: 45.75%\n",
      "Epoch 91 | Train Loss: 0.2449 | Val Loss: 0.2456 | Val Acc: 46.18%\n",
      "Epoch 92 | Train Loss: 0.2465 | Val Loss: 0.2452 | Val Acc: 44.58%\n",
      "Epoch 93 | Train Loss: 0.2436 | Val Loss: 0.2479 | Val Acc: 44.90%\n",
      "Epoch 94 | Train Loss: 0.2457 | Val Loss: 0.2475 | Val Acc: 45.16%\n",
      "Epoch 95 | Train Loss: 0.2444 | Val Loss: 0.2901 | Val Acc: 44.47%\n",
      "Epoch 96 | Train Loss: 0.2439 | Val Loss: 0.2454 | Val Acc: 45.75%\n",
      "Epoch 97 | Train Loss: 0.2423 | Val Loss: 0.2416 | Val Acc: 46.55%\n",
      "Epoch 98 | Train Loss: 0.2431 | Val Loss: 0.2459 | Val Acc: 45.48%\n",
      "Epoch 99 | Train Loss: 0.2428 | Val Loss: 0.2402 | Val Acc: 45.75%\n",
      "Epoch 100 | Train Loss: 0.2421 | Val Loss: 0.2419 | Val Acc: 45.96%\n",
      "Epoch 101 | Train Loss: 0.2419 | Val Loss: 0.2428 | Val Acc: 46.45%\n",
      "Epoch 102 | Train Loss: 0.2410 | Val Loss: 0.2408 | Val Acc: 45.27%\n",
      "Epoch 103 | Train Loss: 0.2420 | Val Loss: 0.2471 | Val Acc: 45.75%\n",
      "Epoch 104 | Train Loss: 0.2413 | Val Loss: 0.2473 | Val Acc: 45.64%\n",
      "Epoch 105 | Train Loss: 0.2402 | Val Loss: 0.2438 | Val Acc: 45.48%\n",
      "Epoch 106 | Train Loss: 0.2398 | Val Loss: 0.2412 | Val Acc: 46.71%\n",
      "Epoch 107 | Train Loss: 0.2401 | Val Loss: 0.2466 | Val Acc: 45.59%\n",
      "Epoch 108 | Train Loss: 0.2401 | Val Loss: 0.2405 | Val Acc: 45.96%\n",
      "Epoch 109 | Train Loss: 0.2397 | Val Loss: 0.2388 | Val Acc: 46.50%\n",
      "Epoch 110 | Train Loss: 0.2385 | Val Loss: 0.2398 | Val Acc: 46.71%\n",
      "Epoch 111 | Train Loss: 0.2384 | Val Loss: 0.2401 | Val Acc: 47.51%\n",
      "Epoch 112 | Train Loss: 0.2384 | Val Loss: 0.2441 | Val Acc: 46.98%\n",
      "Epoch 113 | Train Loss: 0.2376 | Val Loss: 0.2385 | Val Acc: 46.39%\n",
      "Epoch 114 | Train Loss: 0.2384 | Val Loss: 0.2378 | Val Acc: 46.66%\n",
      "Epoch 115 | Train Loss: 0.2364 | Val Loss: 0.2395 | Val Acc: 46.39%\n",
      "Epoch 116 | Train Loss: 0.2364 | Val Loss: 0.2391 | Val Acc: 47.25%\n",
      "Epoch 117 | Train Loss: 0.2358 | Val Loss: 0.2355 | Val Acc: 48.10%\n",
      "Epoch 118 | Train Loss: 0.2356 | Val Loss: 0.2405 | Val Acc: 46.23%\n",
      "Epoch 119 | Train Loss: 0.2357 | Val Loss: 0.2436 | Val Acc: 46.77%\n",
      "Epoch 120 | Train Loss: 0.2361 | Val Loss: 0.2470 | Val Acc: 45.64%\n",
      "Epoch 121 | Train Loss: 0.2353 | Val Loss: 0.2382 | Val Acc: 47.30%\n",
      "Epoch 122 | Train Loss: 0.2346 | Val Loss: 0.2392 | Val Acc: 47.35%\n",
      "Epoch 123 | Train Loss: 0.2342 | Val Loss: 0.2419 | Val Acc: 47.30%\n",
      "Epoch 124 | Train Loss: 0.2343 | Val Loss: 0.2440 | Val Acc: 47.68%\n",
      "Epoch 125 | Train Loss: 0.2338 | Val Loss: 0.2362 | Val Acc: 46.50%\n",
      "Epoch 126 | Train Loss: 0.2336 | Val Loss: 0.2355 | Val Acc: 47.51%\n",
      "Epoch 127 | Train Loss: 0.2338 | Val Loss: 0.2506 | Val Acc: 46.93%\n",
      "Epoch 128 | Train Loss: 0.2320 | Val Loss: 0.2354 | Val Acc: 47.89%\n",
      "Epoch 129 | Train Loss: 0.2325 | Val Loss: 0.2361 | Val Acc: 46.98%\n",
      "Epoch 130 | Train Loss: 0.2324 | Val Loss: 0.2388 | Val Acc: 47.62%\n",
      "Epoch 131 | Train Loss: 0.2309 | Val Loss: 0.2353 | Val Acc: 45.80%\n",
      "Epoch 132 | Train Loss: 0.2310 | Val Loss: 0.2340 | Val Acc: 47.89%\n",
      "Epoch 133 | Train Loss: 0.2317 | Val Loss: 0.2332 | Val Acc: 48.05%\n",
      "Epoch 134 | Train Loss: 0.2314 | Val Loss: 0.2342 | Val Acc: 48.64%\n",
      "Epoch 135 | Train Loss: 0.2302 | Val Loss: 0.2420 | Val Acc: 47.30%\n",
      "Epoch 136 | Train Loss: 0.2296 | Val Loss: 0.2340 | Val Acc: 48.10%\n",
      "Epoch 137 | Train Loss: 0.2299 | Val Loss: 0.2322 | Val Acc: 47.89%\n",
      "Epoch 138 | Train Loss: 0.2292 | Val Loss: 0.2336 | Val Acc: 48.16%\n",
      "Epoch 139 | Train Loss: 0.2292 | Val Loss: 0.2333 | Val Acc: 47.78%\n",
      "Epoch 140 | Train Loss: 0.2289 | Val Loss: 0.2307 | Val Acc: 47.94%\n",
      "Epoch 141 | Train Loss: 0.2293 | Val Loss: 0.2398 | Val Acc: 48.05%\n",
      "Epoch 142 | Train Loss: 0.2281 | Val Loss: 0.2326 | Val Acc: 48.90%\n",
      "Epoch 143 | Train Loss: 0.2273 | Val Loss: 0.2355 | Val Acc: 47.68%\n",
      "Epoch 144 | Train Loss: 0.2278 | Val Loss: 0.2335 | Val Acc: 47.89%\n",
      "Epoch 145 | Train Loss: 0.2280 | Val Loss: 0.2342 | Val Acc: 48.26%\n",
      "Epoch 146 | Train Loss: 0.2261 | Val Loss: 0.2351 | Val Acc: 48.21%\n",
      "Epoch 147 | Train Loss: 0.2272 | Val Loss: 0.2339 | Val Acc: 48.32%\n",
      "Epoch 148 | Train Loss: 0.2267 | Val Loss: 0.2331 | Val Acc: 48.42%\n",
      "Epoch 149 | Train Loss: 0.2270 | Val Loss: 0.2284 | Val Acc: 48.42%\n",
      "Epoch 150 | Train Loss: 0.2255 | Val Loss: 0.2284 | Val Acc: 48.74%\n",
      "Epoch 151 | Train Loss: 0.2259 | Val Loss: 0.2318 | Val Acc: 48.64%\n",
      "Epoch 152 | Train Loss: 0.2255 | Val Loss: 0.2265 | Val Acc: 48.48%\n",
      "Epoch 153 | Train Loss: 0.2249 | Val Loss: 0.2296 | Val Acc: 48.85%\n",
      "Epoch 154 | Train Loss: 0.2251 | Val Loss: 0.2307 | Val Acc: 48.96%\n",
      "Epoch 155 | Train Loss: 0.2247 | Val Loss: 0.2315 | Val Acc: 48.48%\n",
      "Epoch 156 | Train Loss: 0.2245 | Val Loss: 0.2304 | Val Acc: 48.42%\n",
      "Epoch 157 | Train Loss: 0.2228 | Val Loss: 0.2267 | Val Acc: 48.32%\n",
      "Epoch 158 | Train Loss: 0.2239 | Val Loss: 0.2285 | Val Acc: 48.96%\n",
      "Epoch 159 | Train Loss: 0.2233 | Val Loss: 0.2343 | Val Acc: 50.51%\n",
      "Epoch 160 | Train Loss: 0.2226 | Val Loss: 0.2307 | Val Acc: 48.05%\n",
      "Epoch 161 | Train Loss: 0.2231 | Val Loss: 0.2312 | Val Acc: 48.05%\n",
      "Epoch 162 | Train Loss: 0.2238 | Val Loss: 0.2315 | Val Acc: 48.69%\n",
      "Epoch 163 | Train Loss: 0.2235 | Val Loss: 0.2260 | Val Acc: 48.64%\n",
      "Epoch 164 | Train Loss: 0.2229 | Val Loss: 0.2277 | Val Acc: 49.12%\n",
      "Epoch 165 | Train Loss: 0.2218 | Val Loss: 0.2317 | Val Acc: 48.69%\n",
      "Epoch 166 | Train Loss: 0.2233 | Val Loss: 0.2309 | Val Acc: 48.64%\n",
      "Epoch 167 | Train Loss: 0.2210 | Val Loss: 0.2259 | Val Acc: 49.44%\n",
      "Epoch 168 | Train Loss: 0.2215 | Val Loss: 0.2327 | Val Acc: 49.17%\n",
      "Epoch 169 | Train Loss: 0.2217 | Val Loss: 0.2236 | Val Acc: 49.49%\n",
      "Epoch 170 | Train Loss: 0.2209 | Val Loss: 0.2250 | Val Acc: 48.90%\n",
      "Epoch 171 | Train Loss: 0.2211 | Val Loss: 0.2233 | Val Acc: 50.61%\n",
      "Epoch 172 | Train Loss: 0.2199 | Val Loss: 0.2295 | Val Acc: 49.44%\n",
      "Epoch 173 | Train Loss: 0.2200 | Val Loss: 0.2256 | Val Acc: 48.37%\n",
      "Epoch 174 | Train Loss: 0.2203 | Val Loss: 0.2246 | Val Acc: 49.17%\n",
      "Epoch 175 | Train Loss: 0.2204 | Val Loss: 0.2260 | Val Acc: 47.68%\n",
      "Epoch 176 | Train Loss: 0.2203 | Val Loss: 0.2261 | Val Acc: 49.76%\n",
      "Epoch 177 | Train Loss: 0.2199 | Val Loss: 0.2246 | Val Acc: 49.65%\n",
      "Epoch 178 | Train Loss: 0.2183 | Val Loss: 0.2249 | Val Acc: 49.01%\n",
      "Epoch 179 | Train Loss: 0.2185 | Val Loss: 0.2244 | Val Acc: 49.44%\n",
      "Epoch 180 | Train Loss: 0.2185 | Val Loss: 0.2263 | Val Acc: 49.39%\n",
      "Epoch 181 | Train Loss: 0.2192 | Val Loss: 0.2218 | Val Acc: 50.94%\n",
      "Epoch 182 | Train Loss: 0.2192 | Val Loss: 0.2240 | Val Acc: 50.40%\n",
      "Epoch 183 | Train Loss: 0.2178 | Val Loss: 0.2223 | Val Acc: 49.92%\n",
      "Epoch 184 | Train Loss: 0.2176 | Val Loss: 0.2228 | Val Acc: 50.94%\n",
      "Epoch 185 | Train Loss: 0.2175 | Val Loss: 0.2215 | Val Acc: 50.13%\n",
      "Epoch 186 | Train Loss: 0.2161 | Val Loss: 0.2201 | Val Acc: 50.56%\n",
      "Epoch 187 | Train Loss: 0.2176 | Val Loss: 0.2225 | Val Acc: 49.60%\n",
      "Epoch 188 | Train Loss: 0.2173 | Val Loss: 0.2216 | Val Acc: 50.24%\n",
      "Epoch 189 | Train Loss: 0.2177 | Val Loss: 0.2237 | Val Acc: 49.55%\n",
      "Epoch 190 | Train Loss: 0.2158 | Val Loss: 0.2261 | Val Acc: 51.26%\n",
      "Epoch 191 | Train Loss: 0.2156 | Val Loss: 0.2226 | Val Acc: 50.99%\n",
      "Epoch 192 | Train Loss: 0.2171 | Val Loss: 0.2216 | Val Acc: 50.61%\n",
      "Epoch 193 | Train Loss: 0.2157 | Val Loss: 0.2206 | Val Acc: 50.72%\n",
      "Epoch 194 | Train Loss: 0.2151 | Val Loss: 0.2279 | Val Acc: 50.13%\n",
      "Epoch 195 | Train Loss: 0.2155 | Val Loss: 0.2238 | Val Acc: 50.51%\n",
      "Epoch 196 | Train Loss: 0.2157 | Val Loss: 0.2262 | Val Acc: 49.65%\n",
      "Epoch 197 | Train Loss: 0.2160 | Val Loss: 0.2192 | Val Acc: 50.77%\n",
      "Epoch 198 | Train Loss: 0.2141 | Val Loss: 0.2195 | Val Acc: 49.06%\n",
      "Epoch 199 | Train Loss: 0.2134 | Val Loss: 0.2191 | Val Acc: 50.08%\n",
      "Epoch 200 | Train Loss: 0.2145 | Val Loss: 0.2221 | Val Acc: 50.35%\n",
      "Epoch 201 | Train Loss: 0.2150 | Val Loss: 0.2218 | Val Acc: 48.26%\n",
      "Epoch 202 | Train Loss: 0.2142 | Val Loss: 0.2224 | Val Acc: 50.51%\n",
      "Epoch 203 | Train Loss: 0.2139 | Val Loss: 0.2182 | Val Acc: 50.61%\n",
      "Epoch 204 | Train Loss: 0.2140 | Val Loss: 0.2192 | Val Acc: 50.40%\n",
      "Epoch 205 | Train Loss: 0.2130 | Val Loss: 0.2192 | Val Acc: 51.10%\n",
      "Epoch 206 | Train Loss: 0.2139 | Val Loss: 0.2204 | Val Acc: 50.99%\n",
      "Epoch 207 | Train Loss: 0.2143 | Val Loss: 0.2189 | Val Acc: 48.42%\n",
      "Epoch 208 | Train Loss: 0.2133 | Val Loss: 0.2178 | Val Acc: 50.56%\n",
      "Epoch 209 | Train Loss: 0.2128 | Val Loss: 0.2171 | Val Acc: 52.00%\n",
      "Epoch 210 | Train Loss: 0.2120 | Val Loss: 0.2300 | Val Acc: 49.12%\n",
      "Epoch 211 | Train Loss: 0.2124 | Val Loss: 0.2163 | Val Acc: 51.63%\n",
      "Epoch 212 | Train Loss: 0.2123 | Val Loss: 0.2251 | Val Acc: 49.01%\n",
      "Epoch 213 | Train Loss: 0.2108 | Val Loss: 0.2163 | Val Acc: 51.26%\n",
      "Epoch 214 | Train Loss: 0.2112 | Val Loss: 0.2155 | Val Acc: 51.42%\n",
      "Epoch 215 | Train Loss: 0.2105 | Val Loss: 0.2162 | Val Acc: 50.19%\n",
      "Epoch 216 | Train Loss: 0.2108 | Val Loss: 0.2189 | Val Acc: 49.49%\n",
      "Epoch 217 | Train Loss: 0.2113 | Val Loss: 0.2153 | Val Acc: 49.97%\n",
      "Epoch 218 | Train Loss: 0.2100 | Val Loss: 0.2163 | Val Acc: 51.20%\n",
      "Epoch 219 | Train Loss: 0.2118 | Val Loss: 0.2161 | Val Acc: 49.65%\n",
      "Epoch 220 | Train Loss: 0.2091 | Val Loss: 0.2208 | Val Acc: 49.28%\n",
      "Epoch 221 | Train Loss: 0.2098 | Val Loss: 0.2172 | Val Acc: 49.81%\n",
      "Epoch 222 | Train Loss: 0.2105 | Val Loss: 0.2156 | Val Acc: 48.80%\n",
      "Epoch 223 | Train Loss: 0.2102 | Val Loss: 0.2168 | Val Acc: 49.28%\n",
      "Epoch 224 | Train Loss: 0.2093 | Val Loss: 0.2154 | Val Acc: 50.77%\n",
      "Epoch 225 | Train Loss: 0.2096 | Val Loss: 0.2135 | Val Acc: 50.61%\n",
      "Epoch 226 | Train Loss: 0.2105 | Val Loss: 0.2170 | Val Acc: 49.87%\n",
      "Epoch 227 | Train Loss: 0.2108 | Val Loss: 0.2161 | Val Acc: 49.06%\n",
      "Epoch 228 | Train Loss: 0.2099 | Val Loss: 0.2126 | Val Acc: 51.15%\n",
      "Epoch 229 | Train Loss: 0.2089 | Val Loss: 0.2160 | Val Acc: 50.40%\n",
      "Epoch 230 | Train Loss: 0.2085 | Val Loss: 0.2159 | Val Acc: 50.72%\n",
      "Epoch 231 | Train Loss: 0.2107 | Val Loss: 0.2130 | Val Acc: 50.56%\n",
      "Epoch 232 | Train Loss: 0.2083 | Val Loss: 0.2129 | Val Acc: 51.26%\n",
      "Epoch 233 | Train Loss: 0.2081 | Val Loss: 0.2146 | Val Acc: 50.61%\n",
      "Epoch 234 | Train Loss: 0.2095 | Val Loss: 0.2213 | Val Acc: 49.33%\n",
      "Epoch 235 | Train Loss: 0.2089 | Val Loss: 0.2120 | Val Acc: 51.10%\n",
      "Epoch 236 | Train Loss: 0.2064 | Val Loss: 0.2181 | Val Acc: 50.94%\n",
      "Epoch 237 | Train Loss: 0.2079 | Val Loss: 0.2145 | Val Acc: 51.74%\n",
      "Epoch 238 | Train Loss: 0.2090 | Val Loss: 0.2143 | Val Acc: 50.13%\n",
      "Epoch 239 | Train Loss: 0.2079 | Val Loss: 0.2121 | Val Acc: 51.47%\n",
      "Epoch 240 | Train Loss: 0.2081 | Val Loss: 0.2115 | Val Acc: 50.13%\n",
      "Epoch 241 | Train Loss: 0.2072 | Val Loss: 0.2156 | Val Acc: 50.03%\n",
      "Epoch 242 | Train Loss: 0.2080 | Val Loss: 0.2133 | Val Acc: 51.79%\n",
      "Epoch 243 | Train Loss: 0.2071 | Val Loss: 0.2118 | Val Acc: 50.77%\n",
      "Epoch 244 | Train Loss: 0.2068 | Val Loss: 0.2113 | Val Acc: 50.61%\n",
      "Epoch 245 | Train Loss: 0.2059 | Val Loss: 0.2144 | Val Acc: 50.56%\n",
      "Epoch 246 | Train Loss: 0.2067 | Val Loss: 0.2146 | Val Acc: 51.10%\n",
      "Epoch 247 | Train Loss: 0.2071 | Val Loss: 0.2129 | Val Acc: 49.97%\n",
      "Epoch 248 | Train Loss: 0.2053 | Val Loss: 0.2147 | Val Acc: 50.67%\n",
      "Epoch 249 | Train Loss: 0.2069 | Val Loss: 0.2123 | Val Acc: 51.68%\n",
      "Epoch 250 | Train Loss: 0.2063 | Val Loss: 0.2098 | Val Acc: 50.45%\n",
      "Epoch 251 | Train Loss: 0.2058 | Val Loss: 0.2146 | Val Acc: 49.97%\n",
      "Epoch 252 | Train Loss: 0.2048 | Val Loss: 0.2120 | Val Acc: 51.15%\n",
      "Epoch 253 | Train Loss: 0.2055 | Val Loss: 0.2097 | Val Acc: 51.52%\n",
      "Epoch 254 | Train Loss: 0.2051 | Val Loss: 0.2096 | Val Acc: 52.11%\n",
      "Epoch 255 | Train Loss: 0.2048 | Val Loss: 0.2142 | Val Acc: 52.11%\n",
      "Epoch 256 | Train Loss: 0.2049 | Val Loss: 0.2107 | Val Acc: 51.79%\n",
      "Epoch 257 | Train Loss: 0.2059 | Val Loss: 0.2140 | Val Acc: 50.13%\n",
      "Epoch 258 | Train Loss: 0.2046 | Val Loss: 0.2099 | Val Acc: 50.51%\n",
      "Epoch 259 | Train Loss: 0.2051 | Val Loss: 0.2115 | Val Acc: 51.58%\n",
      "Epoch 260 | Train Loss: 0.2034 | Val Loss: 0.2092 | Val Acc: 52.81%\n",
      "Epoch 261 | Train Loss: 0.2040 | Val Loss: 0.2111 | Val Acc: 52.22%\n",
      "Epoch 262 | Train Loss: 0.2046 | Val Loss: 0.2142 | Val Acc: 51.10%\n",
      "Epoch 263 | Train Loss: 0.2046 | Val Loss: 0.2110 | Val Acc: 51.52%\n",
      "Epoch 264 | Train Loss: 0.2042 | Val Loss: 0.2118 | Val Acc: 50.08%\n",
      "Epoch 265 | Train Loss: 0.2044 | Val Loss: 0.2126 | Val Acc: 51.20%\n",
      "Epoch 266 | Train Loss: 0.2038 | Val Loss: 0.2135 | Val Acc: 52.54%\n",
      "Epoch 267 | Train Loss: 0.2041 | Val Loss: 0.2105 | Val Acc: 52.65%\n",
      "Epoch 268 | Train Loss: 0.2035 | Val Loss: 0.2115 | Val Acc: 53.13%\n",
      "Epoch 269 | Train Loss: 0.2025 | Val Loss: 0.2116 | Val Acc: 51.20%\n",
      "Epoch 270 | Train Loss: 0.2024 | Val Loss: 0.2075 | Val Acc: 53.29%\n",
      "Epoch 271 | Train Loss: 0.2028 | Val Loss: 0.2110 | Val Acc: 52.22%\n",
      "Epoch 272 | Train Loss: 0.2033 | Val Loss: 0.2099 | Val Acc: 52.91%\n",
      "Epoch 273 | Train Loss: 0.2024 | Val Loss: 0.2106 | Val Acc: 50.51%\n",
      "Epoch 274 | Train Loss: 0.2013 | Val Loss: 0.2095 | Val Acc: 53.02%\n",
      "Epoch 275 | Train Loss: 0.2033 | Val Loss: 0.2095 | Val Acc: 53.18%\n",
      "Epoch 276 | Train Loss: 0.2025 | Val Loss: 0.2095 | Val Acc: 52.06%\n",
      "Epoch 277 | Train Loss: 0.2017 | Val Loss: 0.2075 | Val Acc: 51.63%\n",
      "Epoch 278 | Train Loss: 0.2017 | Val Loss: 0.2118 | Val Acc: 52.65%\n",
      "Epoch 279 | Train Loss: 0.2024 | Val Loss: 0.2061 | Val Acc: 52.43%\n",
      "Epoch 280 | Train Loss: 0.2040 | Val Loss: 0.2129 | Val Acc: 50.99%\n",
      "Epoch 281 | Train Loss: 0.2019 | Val Loss: 0.2085 | Val Acc: 52.27%\n",
      "Epoch 282 | Train Loss: 0.2018 | Val Loss: 0.2093 | Val Acc: 53.18%\n",
      "Epoch 283 | Train Loss: 0.2014 | Val Loss: 0.2075 | Val Acc: 52.81%\n",
      "Epoch 284 | Train Loss: 0.2039 | Val Loss: 0.2094 | Val Acc: 52.81%\n",
      "Epoch 285 | Train Loss: 0.2019 | Val Loss: 0.2117 | Val Acc: 53.02%\n",
      "Epoch 286 | Train Loss: 0.2025 | Val Loss: 0.2074 | Val Acc: 52.16%\n",
      "Epoch 287 | Train Loss: 0.2011 | Val Loss: 0.2072 | Val Acc: 52.16%\n",
      "Epoch 288 | Train Loss: 0.2005 | Val Loss: 0.2073 | Val Acc: 52.16%\n",
      "Epoch 289 | Train Loss: 0.2013 | Val Loss: 0.2043 | Val Acc: 53.66%\n",
      "Epoch 290 | Train Loss: 0.2007 | Val Loss: 0.2063 | Val Acc: 52.97%\n",
      "Epoch 291 | Train Loss: 0.2001 | Val Loss: 0.2096 | Val Acc: 53.45%\n",
      "Epoch 292 | Train Loss: 0.2015 | Val Loss: 0.2066 | Val Acc: 53.55%\n",
      "Epoch 293 | Train Loss: 0.2007 | Val Loss: 0.2079 | Val Acc: 51.63%\n",
      "Epoch 294 | Train Loss: 0.2002 | Val Loss: 0.2108 | Val Acc: 53.18%\n",
      "Epoch 295 | Train Loss: 0.2000 | Val Loss: 0.2085 | Val Acc: 52.22%\n",
      "Epoch 296 | Train Loss: 0.1992 | Val Loss: 0.2130 | Val Acc: 52.27%\n",
      "Epoch 297 | Train Loss: 0.2002 | Val Loss: 0.2074 | Val Acc: 53.13%\n",
      "Epoch 298 | Train Loss: 0.1988 | Val Loss: 0.2027 | Val Acc: 52.70%\n",
      "Epoch 299 | Train Loss: 0.2000 | Val Loss: 0.2038 | Val Acc: 52.38%\n",
      "Epoch 300 | Train Loss: 0.1988 | Val Loss: 0.2105 | Val Acc: 53.34%\n",
      "Epoch 301 | Train Loss: 0.1993 | Val Loss: 0.2052 | Val Acc: 51.26%\n",
      "Epoch 302 | Train Loss: 0.1992 | Val Loss: 0.2079 | Val Acc: 52.00%\n",
      "Epoch 303 | Train Loss: 0.1990 | Val Loss: 0.2058 | Val Acc: 53.23%\n",
      "Epoch 304 | Train Loss: 0.1987 | Val Loss: 0.2078 | Val Acc: 53.50%\n",
      "Epoch 305 | Train Loss: 0.1988 | Val Loss: 0.2091 | Val Acc: 53.61%\n",
      "Epoch 306 | Train Loss: 0.1987 | Val Loss: 0.2053 | Val Acc: 53.50%\n",
      "Epoch 307 | Train Loss: 0.1980 | Val Loss: 0.2047 | Val Acc: 52.97%\n",
      "Epoch 308 | Train Loss: 0.1984 | Val Loss: 0.2067 | Val Acc: 53.02%\n",
      "Epoch 309 | Train Loss: 0.1981 | Val Loss: 0.2050 | Val Acc: 54.14%\n",
      "Epoch 310 | Train Loss: 0.1976 | Val Loss: 0.2043 | Val Acc: 52.91%\n",
      "Epoch 311 | Train Loss: 0.1993 | Val Loss: 0.2034 | Val Acc: 53.87%\n",
      "Epoch 312 | Train Loss: 0.1972 | Val Loss: 0.2090 | Val Acc: 52.59%\n",
      "Epoch 313 | Train Loss: 0.1975 | Val Loss: 0.2092 | Val Acc: 54.30%\n",
      "Epoch 314 | Train Loss: 0.1979 | Val Loss: 0.2160 | Val Acc: 53.77%\n",
      "Epoch 315 | Train Loss: 0.1979 | Val Loss: 0.2156 | Val Acc: 54.36%\n",
      "Epoch 316 | Train Loss: 0.1978 | Val Loss: 0.2065 | Val Acc: 52.75%\n",
      "Epoch 317 | Train Loss: 0.1969 | Val Loss: 0.2072 | Val Acc: 53.45%\n",
      "Epoch 318 | Train Loss: 0.1988 | Val Loss: 0.2037 | Val Acc: 51.68%\n",
      "Training stopped at epoch 318.\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "counter = 0\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = criterion(out, batch.y.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss = criterion(out, batch.y.argmax(dim=1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds, true = split_batch_by_molecule(out, batch)\n",
    "            for p, t in zip(preds, true):\n",
    "                if torch.equal(p, t):\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # validation accuracy\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        counter = 0\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), f\"../models/egat/egat_{epoch+1}.pt\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Training stopped at epoch {epoch+1}.\")\n",
    "            break"
   ],
   "id": "afa361e1ba1ba89f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T09:27:11.064712Z",
     "start_time": "2025-11-22T09:27:05.925878Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1121\n",
      "Test Accuracy: 72.55%\n",
      "Saved predictions to ../results/gine_test_eval.csv.\n"
     ]
    }
   ],
   "execution_count": 14,
   "source": [
    "MODEL_PATH = '../models/egat/egat_156.pt'\n",
    "OUTPUT_CSV = '../results/egat_test_eval.csv'\n",
    "\n",
    "# load best model\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "smiles_original = []\n",
    "smiles_predicted = []\n",
    "smiles_matched = []\n",
    "\n",
    "# test evaluation\n",
    "model.eval()\n",
    "test_loss, correct, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        out = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "        loss = criterion(out, batch.y.argmax(dim=1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        preds, true = split_batch_by_molecule(out, batch)\n",
    "        for p, t in zip(preds, true):\n",
    "            if torch.equal(p, t):\n",
    "                smiles_matched.append(True)\n",
    "                correct += 1\n",
    "            else:\n",
    "                smiles_matched.append(False)\n",
    "            total += 1\n",
    "\n",
    "        smiles_original.extend(batch.smiles)\n",
    "        smiles_predicted.extend(get_prediction_smiles(preds, batch.smiles))\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = correct / total\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# saves test predictions to .csv file\n",
    "test_df = pd.DataFrame({\n",
    "    'SMILES Original': smiles_original,\n",
    "    'SMILES Predicted': smiles_predicted,\n",
    "    'SMILES Matched': smiles_matched\n",
    "})\n",
    "test_df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved predictions to {OUTPUT_CSV}.\")"
   ],
   "id": "b0a183483266ebd0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T12:36:43.403549Z",
     "start_time": "2025-11-21T12:36:43.400882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 10 correct: 84.48%\n",
    "# 11 correct: 96.25%\n",
    "# 20 correct: 91.97%\n",
    "# 21 correct: 88.92%"
   ],
   "id": "9840146f361fb896",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0f95fb7c2f44fc3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
